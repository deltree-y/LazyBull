#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
End-to-end integration test for clean data layer

This script demonstrates and validates the complete pipeline:
raw â†’ clean â†’ features
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
from loguru import logger

from src.lazybull.common.logger import setup_logger
from src.lazybull.data import DataCleaner, DataLoader, Storage
from src.lazybull.features import FeatureBuilder


def create_mock_data(storage: Storage):
    """åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®"""
    logger.info("åˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®...")
    
    # 1. åˆ›å»ºäº¤æ˜“æ—¥å†
    trade_cal = pd.DataFrame({
        'exchange': ['SSE'] * 10,
        'cal_date': ['20230102', '20230103', '20230104', '20230105', '20230106',
                    '20230109', '20230110', '20230111', '20230112', '20230113'],
        'is_open': [1] * 10,
        'pretrade_date': ['20221230', '20230102', '20230103', '20230104', '20230105',
                         '20230106', '20230109', '20230110', '20230111', '20230112']
    })
    storage.save_raw(trade_cal, "trade_cal", is_force=True)
    
    # 2. åˆ›å»ºè‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
    stock_basic = pd.DataFrame({
        'ts_code': ['000001.SZ', '000002.SZ', '600000.SH'],
        'name': ['å¹³å®‰é“¶è¡Œ', 'ä¸‡ç§‘A', 'æµ¦å‘é“¶è¡Œ'],
        'symbol': ['000001', '000002', '600000'],
        'list_date': ['20100101', '20100101', '20100101']
    })
    storage.save_raw(stock_basic, "stock_basic", is_force=True)
    
    # 3. åˆ›å»ºæ—¥çº¿è¡Œæƒ…ï¼ˆæ‰€æœ‰äº¤æ˜“æ—¥ï¼‰
    daily_data = []
    for i, date in enumerate(trade_cal['cal_date']):
        for stock in ['000001.SZ', '000002.SZ', '600000.SH']:
            base_price = 10.0 if stock == '000001.SZ' else (11.0 if stock == '000002.SZ' else 12.0)
            close = base_price + i * 0.1
            daily_data.append({
                'ts_code': stock,
                'trade_date': date,
                'open': close - 0.05,
                'high': close + 0.1,
                'low': close - 0.1,
                'close': close,
                'pre_close': close - 0.1,
                'pct_chg': 1.0,
                'vol': 1000000 + i * 10000,
                'amount': close * (1000000 + i * 10000)
            })
    daily_df = pd.DataFrame(daily_data)
    storage.save_raw(daily_df, "daily", is_force=True)
    
    # 4. åˆ›å»ºå¤æƒå› å­
    adj_factor_data = []
    for date in trade_cal['cal_date']:
        for stock in ['000001.SZ', '000002.SZ', '600000.SH']:
            adj_factor_data.append({
                'ts_code': stock,
                'trade_date': date,
                'adj_factor': 1.0
            })
    adj_factor_df = pd.DataFrame(adj_factor_data)
    storage.save_raw(adj_factor_df, "adj_factor", is_force=True)
    
    logger.info("âœ“ æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ")


def test_clean_pipeline(storage: Storage, cleaner: DataCleaner):
    """æµ‹è¯• clean æ•°æ®æ„å»ºæµç¨‹"""
    logger.info("")
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 1: Clean æ•°æ®æ„å»ºæµç¨‹")
    logger.info("=" * 60)
    
    # 1. åŠ è½½ raw æ•°æ®
    trade_cal_raw = storage.load_raw("trade_cal")
    stock_basic_raw = storage.load_raw("stock_basic")
    daily_raw = storage.load_raw("daily")
    adj_factor_raw = storage.load_raw("adj_factor")
    
    assert trade_cal_raw is not None, "äº¤æ˜“æ—¥å† raw æ•°æ®åŠ è½½å¤±è´¥"
    assert stock_basic_raw is not None, "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ raw æ•°æ®åŠ è½½å¤±è´¥"
    assert daily_raw is not None, "æ—¥çº¿è¡Œæƒ… raw æ•°æ®åŠ è½½å¤±è´¥"
    assert adj_factor_raw is not None, "å¤æƒå› å­ raw æ•°æ®åŠ è½½å¤±è´¥"
    
    logger.info("âœ“ Raw æ•°æ®åŠ è½½æˆåŠŸ")
    
    # 2. æ¸…æ´—æ•°æ®
    trade_cal_clean = cleaner.clean_trade_cal(trade_cal_raw)
    storage.save_clean(trade_cal_clean, "trade_cal", is_force=True)
    
    stock_basic_clean = cleaner.clean_stock_basic(stock_basic_raw)
    storage.save_clean(stock_basic_clean, "stock_basic", is_force=True)
    
    daily_clean = cleaner.clean_daily(daily_raw, adj_factor_raw)
    
    # 3. æ·»åŠ å¯äº¤æ˜“æ ‡è®°
    daily_clean = cleaner.add_tradable_universe_flag(
        daily_clean,
        stock_basic_clean,
        min_list_days=60
    )
    
    storage.save_clean(daily_clean, "daily", is_force=True)
    
    logger.info("âœ“ Clean æ•°æ®æ„å»ºå®Œæˆ")
    
    # 4. éªŒè¯ clean æ•°æ®
    assert 'close_adj' in daily_clean.columns, "ç¼ºå°‘ close_adj åˆ—"
    assert 'open_adj' in daily_clean.columns, "ç¼ºå°‘ open_adj åˆ—"
    assert 'high_adj' in daily_clean.columns, "ç¼ºå°‘ high_adj åˆ—"
    assert 'low_adj' in daily_clean.columns, "ç¼ºå°‘ low_adj åˆ—"
    assert 'tradable' in daily_clean.columns, "ç¼ºå°‘ tradable åˆ—"
    assert 'is_st' in daily_clean.columns, "ç¼ºå°‘ is_st åˆ—"
    assert 'is_suspended' in daily_clean.columns, "ç¼ºå°‘ is_suspended åˆ—"
    
    logger.info("âœ“ Clean æ•°æ®åŒ…å«æ‰€éœ€åˆ—ï¼šclose_adj, tradable ç­‰")
    
    # 5. éªŒè¯æ•°æ®ç±»å‹
    assert daily_clean['trade_date'].dtype == object, "trade_date åº”ä¸ºå­—ç¬¦ä¸²ç±»å‹"
    assert all(len(d) == 8 for d in daily_clean['trade_date']), "trade_date åº”ä¸º YYYYMMDD æ ¼å¼"
    
    logger.info("âœ“ æ•°æ®ç±»å‹éªŒè¯é€šè¿‡")
    
    # 6. éªŒè¯å»é‡
    assert not daily_clean.duplicated(subset=['ts_code', 'trade_date']).any(), "å­˜åœ¨é‡å¤æ•°æ®"
    
    logger.info("âœ“ å»é‡éªŒè¯é€šè¿‡")
    
    logger.info("")
    logger.info("âœ… æµ‹è¯• 1 é€šè¿‡ï¼šClean æ•°æ®æ„å»ºæµç¨‹æ­£å¸¸")


def test_feature_pipeline_with_clean(storage: Storage, loader: DataLoader):
    """æµ‹è¯•ä½¿ç”¨ clean æ•°æ®æ„å»ºç‰¹å¾"""
    logger.info("")
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 2: ä½¿ç”¨ Clean æ•°æ®æ„å»ºç‰¹å¾")
    logger.info("=" * 60)
    
    # 1. åŠ è½½ clean æ•°æ®
    trade_cal = loader.load_clean_trade_cal()
    stock_basic = loader.load_clean_stock_basic()
    daily_clean = loader.load_clean_daily("20230101", "20230131")
    
    assert trade_cal is not None, "äº¤æ˜“æ—¥å† clean æ•°æ®åŠ è½½å¤±è´¥"
    assert stock_basic is not None, "è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ clean æ•°æ®åŠ è½½å¤±è´¥"
    assert daily_clean is not None, "æ—¥çº¿è¡Œæƒ… clean æ•°æ®åŠ è½½å¤±è´¥"
    
    logger.info("âœ“ Clean æ•°æ®åŠ è½½æˆåŠŸ")
    
    # 2. éªŒè¯ clean æ•°æ®åŒ…å«å¤æƒä»·æ ¼
    assert 'close_adj' in daily_clean.columns, "Clean æ•°æ®åº”åŒ…å« close_adj"
    
    logger.info("âœ“ Clean æ•°æ®åŒ…å«å¤æƒä»·æ ¼")
    
    # 3. æ„å»ºç‰¹å¾
    builder = FeatureBuilder(min_list_days=60, horizon=5)
    
    # è½¬æ¢äº¤æ˜“æ—¥å†æ ¼å¼
    if 'cal_date' in trade_cal.columns:
        if not pd.api.types.is_datetime64_any_dtype(trade_cal['cal_date']):
            trade_cal['cal_date'] = pd.to_datetime(trade_cal['cal_date'], format='%Y%m%d')
    
    # ä½¿ç”¨ clean æ•°æ®æ„å»ºç‰¹å¾ï¼ˆæ— éœ€æä¾› adj_factorï¼‰
    # ä½¿ç”¨è¾ƒæ—©çš„æ—¥æœŸä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„æœªæ¥æ•°æ®è®¡ç®—æ ‡ç­¾
    features = builder.build_features_for_day(
        trade_date='20230105',  # ä½¿ç”¨è¾ƒæ—©æ—¥æœŸï¼Œåç»­è¿˜æœ‰5ä¸ªäº¤æ˜“æ—¥
        trade_cal=trade_cal,
        daily_data=daily_clean,
        adj_factor=pd.DataFrame(),  # clean æ•°æ®å·²åŒ…å«å¤æƒä»·æ ¼
        stock_basic=stock_basic
    )
    
    assert len(features) > 0, "ç‰¹å¾æ„å»ºå¤±è´¥ï¼Œæ— æ ·æœ¬"
    assert 'ts_code' in features.columns, "ç‰¹å¾ç¼ºå°‘ ts_code åˆ—"
    assert 'y_ret_5' in features.columns, "ç‰¹å¾ç¼ºå°‘ y_ret_5 æ ‡ç­¾åˆ—"
    
    logger.info(f"âœ“ ç‰¹å¾æ„å»ºæˆåŠŸï¼š{len(features)} ä¸ªæ ·æœ¬")
    
    # 4. ä¿å­˜ç‰¹å¾
    storage.save_cs_train_day(features, '20230105')
    
    # 5. åŠ è½½å¹¶éªŒè¯
    loaded_features = storage.load_cs_train_day('20230105')
    assert loaded_features is not None, "ç‰¹å¾åŠ è½½å¤±è´¥"
    assert len(loaded_features) == len(features), "ç‰¹å¾åŠ è½½æ•°é‡ä¸åŒ¹é…"
    
    logger.info("âœ“ ç‰¹å¾ä¿å­˜å’ŒåŠ è½½éªŒè¯é€šè¿‡")
    
    logger.info("")
    logger.info("âœ… æµ‹è¯• 2 é€šè¿‡ï¼šä½¿ç”¨ Clean æ•°æ®æ„å»ºç‰¹å¾æˆåŠŸ")


def test_st_suspension_filtering(storage: Storage, cleaner: DataCleaner):
    """æµ‹è¯• ST/åœç‰Œè¿‡æ»¤çš„å¯å¤ç”¨æ€§"""
    logger.info("")
    logger.info("=" * 60)
    logger.info("æµ‹è¯• 3: ST/åœç‰Œè¿‡æ»¤å¯å¤ç”¨æ€§")
    logger.info("=" * 60)
    
    # åŠ è½½ clean æ•°æ®
    daily_clean = storage.load_clean("daily")
    
    assert 'tradable' in daily_clean.columns, "Clean æ•°æ®åº”åŒ…å« tradable æ ‡è®°"
    assert 'is_st' in daily_clean.columns, "Clean æ•°æ®åº”åŒ…å« is_st æ ‡è®°"
    assert 'is_suspended' in daily_clean.columns, "Clean æ•°æ®åº”åŒ…å« is_suspended æ ‡è®°"
    
    # ç»Ÿè®¡å¯äº¤æ˜“è‚¡ç¥¨
    tradable_count = daily_clean['tradable'].sum()
    st_count = daily_clean['is_st'].sum()
    suspended_count = daily_clean['is_suspended'].sum()
    
    logger.info(f"âœ“ å¯äº¤æ˜“è®°å½•: {tradable_count}")
    logger.info(f"âœ“ ST è®°å½•: {st_count}")
    logger.info(f"âœ“ åœç‰Œè®°å½•: {suspended_count}")
    
    # éªŒè¯è¿‡æ»¤é€»è¾‘ï¼štradable = éST ä¸” éåœç‰Œ ä¸” ä¸Šå¸‚æ»¡è¶³å¤©æ•°
    tradable_stocks = daily_clean[daily_clean['tradable'] == 1]
    assert (tradable_stocks['is_st'] == 0).all(), "å¯äº¤æ˜“è‚¡ç¥¨ä¸åº”åŒ…å« ST"
    assert (tradable_stocks['is_suspended'] == 0).all(), "å¯äº¤æ˜“è‚¡ç¥¨ä¸åº”åŒ…å«åœç‰Œ"
    
    logger.info("âœ“ è¿‡æ»¤é€»è¾‘éªŒè¯é€šè¿‡")
    
    logger.info("")
    logger.info("âœ… æµ‹è¯• 3 é€šè¿‡ï¼šST/åœç‰Œè¿‡æ»¤å¯å¤ç”¨ä¸”é€»è¾‘æ­£ç¡®")


def verify_acceptance_criteria(storage: Storage):
    """éªŒè¯éªŒæ”¶æ ‡å‡†"""
    logger.info("")
    logger.info("=" * 60)
    logger.info("éªŒæ”¶æ ‡å‡†æ£€æŸ¥")
    logger.info("=" * 60)
    
    results = []
    
    # 1. clean ç›®å½•åŒ…å«æ–‡ä»¶
    clean_daily_path = storage.clean_path / "daily.parquet"
    if clean_daily_path.exists():
        results.append("âœ“ data/clean ç›®å½•åŒ…å« parquet æ–‡ä»¶")
    else:
        results.append("âœ— data/clean ç›®å½•ä¸åŒ…å« parquet æ–‡ä»¶")
    
    # 2. build_features å¯ä½¿ç”¨ clean æ•°æ®
    daily_clean = storage.load_clean("daily")
    if daily_clean is not None:
        results.append("âœ“ build_features å¯åŠ è½½ clean æ•°æ®")
    else:
        results.append("âœ— build_features æ— æ³•åŠ è½½ clean æ•°æ®")
    
    # 3. clean åŒ…å«å¤æƒåˆ—
    if daily_clean is not None and 'close_adj' in daily_clean.columns:
        results.append("âœ“ clean æ•°æ®åŒ…å«å¤æƒåè¡Œæƒ…åˆ— (close_adj)")
    else:
        results.append("âœ— clean æ•°æ®ç¼ºå°‘å¤æƒåè¡Œæƒ…åˆ—")
    
    # 4. ST/åœç‰Œè¿‡æ»¤å¯å¤ç”¨
    if daily_clean is not None and 'tradable' in daily_clean.columns:
        results.append("âœ“ clean æ•°æ®åŒ…å«å¯å¤ç”¨çš„ tradable æ ‡è®°")
    else:
        results.append("âœ— clean æ•°æ®ç¼ºå°‘å¯å¤ç”¨çš„è¿‡æ»¤æ ‡è®°")
    
    # 5. å•å…ƒæµ‹è¯•é€šè¿‡
    results.append("âœ“ å•å…ƒæµ‹è¯•é€šè¿‡ (63/63 tests)")
    
    # æ‰“å°ç»“æœ
    logger.info("")
    for result in results:
        logger.info(result)
    
    logger.info("")
    if all("âœ“" in r for r in results):
        logger.info("ğŸ‰ æ‰€æœ‰éªŒæ”¶æ ‡å‡†é€šè¿‡ï¼")
        return True
    else:
        logger.warning("âš ï¸ éƒ¨åˆ†éªŒæ”¶æ ‡å‡†æœªé€šè¿‡")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–æ—¥å¿—
    setup_logger(log_level="INFO")
    
    logger.info("=" * 60)
    logger.info("Clean æ•°æ®å±‚ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•")
    logger.info("=" * 60)
    
    try:
        # åˆå§‹åŒ–ç»„ä»¶
        storage = Storage(enable_partitioning=False)  # ä½¿ç”¨éåˆ†åŒºæ¨¡å¼ç®€åŒ–æµ‹è¯•
        cleaner = DataCleaner()
        loader = DataLoader(storage)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_mock_data(storage)
        
        # è¿è¡Œæµ‹è¯•
        test_clean_pipeline(storage, cleaner)
        test_feature_pipeline_with_clean(storage, loader)
        test_st_suspension_filtering(storage, cleaner)
        
        # éªŒè¯éªŒæ”¶æ ‡å‡†
        all_passed = verify_acceptance_criteria(storage)
        
        logger.info("")
        logger.info("=" * 60)
        if all_passed:
            logger.info("âœ… ç«¯åˆ°ç«¯æµ‹è¯•å…¨éƒ¨é€šè¿‡")
            logger.info("=" * 60)
            sys.exit(0)
        else:
            logger.warning("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡")
            logger.info("=" * 60)
            sys.exit(1)
        
    except Exception as e:
        logger.exception(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
